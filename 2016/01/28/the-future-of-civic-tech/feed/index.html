<?xml version="1.0" encoding="UTF-8"?><rss version="2.0"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:atom="http://www.w3.org/2005/Atom"
	xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
	xmlns:georss="http://www.georss.org/georss" xmlns:geo="http://www.w3.org/2003/01/geo/wgs84_pos#" xmlns:media="http://search.yahoo.com/mrss/"
	
	>
<channel>
	<title>
	Comments on: The Future of Civic Tech	</title>
	<atom:link href="https://civic.io/2016/01/28/the-future-of-civic-tech/feed/" rel="self" type="application/rss+xml" />
	<link>https://civic.io/2016/01/28/the-future-of-civic-tech/</link>
	<description>The Future is Open</description>
	<lastBuildDate>Fri, 01 Apr 2016 20:15:15 +0000</lastBuildDate>
	<sy:updatePeriod>
	hourly	</sy:updatePeriod>
	<sy:updateFrequency>
	1	</sy:updateFrequency>
	<generator>http://wordpress.com/</generator>
	<item>
		<title>
		By: Mark Boyd (@mgboydcom)		</title>
		<link>https://civic.io/2016/01/28/the-future-of-civic-tech/#comment-10856</link>

		<dc:creator><![CDATA[Mark Boyd (@mgboydcom)]]></dc:creator>
		<pubDate>Fri, 01 Apr 2016 20:15:15 +0000</pubDate>
		<guid isPermaLink="false">http://civic.io/?p=4197#comment-10856</guid>

					<description><![CDATA[Thanks for raising this topic again Mark. In Europe, there is some momentum behind the Open and Agile Smart Cities initiative which is trying to create a kind of organic, bottom-up type model that starts with a civic tech prototype being built in one city, then it is prototyped in a second city, with the aim of seeing what is the minimum core data model that would be necessary to scale it across different jurisdictions. (I wrote about it here: https://neurope.eu/article/addressing-the-fragmentation-of-the-smart-cities-market/). The UK Future Catapult and British Standards Institute are also working on a civic standards framework: http://futurecities.catapult.org.uk/project/cities-standards-institute/

And while i try to keep up with your writing, i only came across this post while researching whether Open311 is still a thing. Open311.org doesn&#039;t show much sign of current life (although there was a teleconference in March evidently), although cities around the world are still adopting the standard: Pittsburgh has recently put through a Council proposal to adopt the standard, and Lewisham in the UK just adopted it: https://www.mysociety.org/2016/02/15/lewisham-council-switches-to-open311/).

I&#039;m kind of hopeful that APIs will help resolve this in a practical sense. With open data portals increasingly enabling the data they host to also be consumed in an API format (Socrata, OpenDataSoft and Junar all do this i believe), then the API means that data is at least available in a standardized (consumable) format. I&#039;m hopeful we will then see civic tech create abstraction layers that mean different open data sources could be consumed in the same way. A bit like how Segment.io and SumAll consume a range of social media APIs and match up the different ways each social media data source configures time or describes a location so that you can create a realtime dashboard across the range of sources. Orchestrate.io do that for databases: funnelling data from multiple databases into the one API. So if open data portals all publish the data sources so that they can be consumed as an API, an application maker could scale across different jurisdictions by consuming each data source via API. From talking with providers like SumAll, that is still a lot of work, though. This is a bit like what @tmcw is saying above, i think. There is still a lot of parsing involved in this, but a lot less than if dealing directly with open data sources in a wider variety of formats, i believe.]]></description>
			<content:encoded><![CDATA[<p>Thanks for raising this topic again Mark. In Europe, there is some momentum behind the Open and Agile Smart Cities initiative which is trying to create a kind of organic, bottom-up type model that starts with a civic tech prototype being built in one city, then it is prototyped in a second city, with the aim of seeing what is the minimum core data model that would be necessary to scale it across different jurisdictions. (I wrote about it here: <a href="https://neurope.eu/article/addressing-the-fragmentation-of-the-smart-cities-market/" rel="nofollow ugc">https://neurope.eu/article/addressing-the-fragmentation-of-the-smart-cities-market/</a>). The UK Future Catapult and British Standards Institute are also working on a civic standards framework: <a href="http://futurecities.catapult.org.uk/project/cities-standards-institute/" rel="nofollow ugc">http://futurecities.catapult.org.uk/project/cities-standards-institute/</a></p>
<p>And while i try to keep up with your writing, i only came across this post while researching whether Open311 is still a thing. Open311.org doesn&#8217;t show much sign of current life (although there was a teleconference in March evidently), although cities around the world are still adopting the standard: Pittsburgh has recently put through a Council proposal to adopt the standard, and Lewisham in the UK just adopted it: <a href="https://www.mysociety.org/2016/02/15/lewisham-council-switches-to-open311/" rel="nofollow ugc">https://www.mysociety.org/2016/02/15/lewisham-council-switches-to-open311/</a>).</p>
<p>I&#8217;m kind of hopeful that APIs will help resolve this in a practical sense. With open data portals increasingly enabling the data they host to also be consumed in an API format (Socrata, OpenDataSoft and Junar all do this i believe), then the API means that data is at least available in a standardized (consumable) format. I&#8217;m hopeful we will then see civic tech create abstraction layers that mean different open data sources could be consumed in the same way. A bit like how Segment.io and SumAll consume a range of social media APIs and match up the different ways each social media data source configures time or describes a location so that you can create a realtime dashboard across the range of sources. Orchestrate.io do that for databases: funnelling data from multiple databases into the one API. So if open data portals all publish the data sources so that they can be consumed as an API, an application maker could scale across different jurisdictions by consuming each data source via API. From talking with providers like SumAll, that is still a lot of work, though. This is a bit like what @tmcw is saying above, i think. There is still a lot of parsing involved in this, but a lot less than if dealing directly with open data sources in a wider variety of formats, i believe.</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: jqnatividad		</title>
		<link>https://civic.io/2016/01/28/the-future-of-civic-tech/#comment-10741</link>

		<dc:creator><![CDATA[jqnatividad]]></dc:creator>
		<pubDate>Tue, 09 Feb 2016 20:43:38 +0000</pubDate>
		<guid isPermaLink="false">http://civic.io/?p=4197#comment-10741</guid>

					<description><![CDATA[What is the &quot;killer app&quot; of open data?  Does it need one? Does Open Data need a &quot;VisiCalc&quot; - the app that turned the Apple II from a hobbyist kit to a business necessity?

GTFS, it can be argued, was the VisiCalc of public transit - but then again, even though we all benefit from it, its primary users and direct consumers are only a handful of specialized organizations.

Do we need a &quot;killer app&quot; for each segment - one for policy-makers (something to help them understand budgets perhaps?), one for citizens (a more robust, performant CityGram on steroids maybe?), and one for businesses (market research, real estate dashboards, etc.)

What about the fact that a lot of incumbent information brokers&#039; business models rely on data being hard to get?  Do we just make it easier for them to consume the data, and hopefully spawn a whole host of govtech/civictech startups to disrupt the incumbents and democratize access to data?

If you look at the &quot;standards&quot; that made the Web successful, one notices that they were unencumbered, and more importantly, easy to implement.  This allowed for experimentation without permission.   Easy enough, so a young college kid halfway around the world cut-and-paste a website, replete with animated Java applets, that brought down a Sun SPARC server in the US (me - in my early days of experimentation).

And speaking of the Web, TBL&#039;s vision of the Semantic Web is enthralling.  But who&#039;s going to do all that markup so we can have the web of linked open data, readily cross-walking disparate seeming unrelated datasets?

To me, open data is a process.  A data portal does not an open data program make.  And as you pointed out, each jurisdiction has its own process, and appropriately so.

And maybe, that&#039;s why &quot;de facto&quot; standards is the main way to go - implement something that works with an easily demonstrable public benefit FIRST, as was the case with GTFS. Or another de facto standard - schema.org, as publishers want to be found in Google&#039;s web.

Of course, there&#039;s also a way unique to government to enforce standards - the force of law, as is the case with XBRL.]]></description>
			<content:encoded><![CDATA[<p>What is the &#8220;killer app&#8221; of open data?  Does it need one? Does Open Data need a &#8220;VisiCalc&#8221; &#8211; the app that turned the Apple II from a hobbyist kit to a business necessity?</p>
<p>GTFS, it can be argued, was the VisiCalc of public transit &#8211; but then again, even though we all benefit from it, its primary users and direct consumers are only a handful of specialized organizations.</p>
<p>Do we need a &#8220;killer app&#8221; for each segment &#8211; one for policy-makers (something to help them understand budgets perhaps?), one for citizens (a more robust, performant CityGram on steroids maybe?), and one for businesses (market research, real estate dashboards, etc.)</p>
<p>What about the fact that a lot of incumbent information brokers&#8217; business models rely on data being hard to get?  Do we just make it easier for them to consume the data, and hopefully spawn a whole host of govtech/civictech startups to disrupt the incumbents and democratize access to data?</p>
<p>If you look at the &#8220;standards&#8221; that made the Web successful, one notices that they were unencumbered, and more importantly, easy to implement.  This allowed for experimentation without permission.   Easy enough, so a young college kid halfway around the world cut-and-paste a website, replete with animated Java applets, that brought down a Sun SPARC server in the US (me &#8211; in my early days of experimentation).</p>
<p>And speaking of the Web, TBL&#8217;s vision of the Semantic Web is enthralling.  But who&#8217;s going to do all that markup so we can have the web of linked open data, readily cross-walking disparate seeming unrelated datasets?</p>
<p>To me, open data is a process.  A data portal does not an open data program make.  And as you pointed out, each jurisdiction has its own process, and appropriately so.</p>
<p>And maybe, that&#8217;s why &#8220;de facto&#8221; standards is the main way to go &#8211; implement something that works with an easily demonstrable public benefit FIRST, as was the case with GTFS. Or another de facto standard &#8211; schema.org, as publishers want to be found in Google&#8217;s web.</p>
<p>Of course, there&#8217;s also a way unique to government to enforce standards &#8211; the force of law, as is the case with XBRL.</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: jpmckinney		</title>
		<link>https://civic.io/2016/01/28/the-future-of-civic-tech/#comment-10725</link>

		<dc:creator><![CDATA[jpmckinney]]></dc:creator>
		<pubDate>Sun, 31 Jan 2016 05:06:15 +0000</pubDate>
		<guid isPermaLink="false">http://civic.io/?p=4197#comment-10725</guid>

					<description><![CDATA[I don&#039;t know what the solution is to getting more standards. There seem to be a relatively small number of people with the skills and experience to develop them, and an even smaller number of candidate organizations to host, curate, promote and enforce the standards. Because the pool is so small, it&#039;s a challenge for those governments and funders who understand the importance of standards to choose initiatives to invest in. Standards also fail really easily, and really slowly, so both funders and initiatives are less likely to try a lot of things, get quick feedback, and learn from what sticks; the timeline is too long and expensive. As a result, you often see the same people involved in standards initiatives (myself included), which limits the number of people who will eventually acquire the skills and experience to lead initiatives. I&#039;ve been thinking about what a curriculum for standards development would look like, as this may be one way out of this problem.

I know Andrew Nicklin at the Center for Government Excellence (to which you linked) is interested in exploring how we can lower the barrier to standards development and reduce the time required. I&#039;m concerned about how such a process could maintain good quality and durability for the standards it produces, but it&#039;s an avenue worth exploring - especially in the absence of any compelling alternative.

By the way - Open Civic Data may be dormant at the moment, but the standard it largely adopted, Popolo (which I lead), is still actively used and developed, by governments, corporations and especially civil society.]]></description>
			<content:encoded><![CDATA[<p>I don&#8217;t know what the solution is to getting more standards. There seem to be a relatively small number of people with the skills and experience to develop them, and an even smaller number of candidate organizations to host, curate, promote and enforce the standards. Because the pool is so small, it&#8217;s a challenge for those governments and funders who understand the importance of standards to choose initiatives to invest in. Standards also fail really easily, and really slowly, so both funders and initiatives are less likely to try a lot of things, get quick feedback, and learn from what sticks; the timeline is too long and expensive. As a result, you often see the same people involved in standards initiatives (myself included), which limits the number of people who will eventually acquire the skills and experience to lead initiatives. I&#8217;ve been thinking about what a curriculum for standards development would look like, as this may be one way out of this problem.</p>
<p>I know Andrew Nicklin at the Center for Government Excellence (to which you linked) is interested in exploring how we can lower the barrier to standards development and reduce the time required. I&#8217;m concerned about how such a process could maintain good quality and durability for the standards it produces, but it&#8217;s an avenue worth exploring &#8211; especially in the absence of any compelling alternative.</p>
<p>By the way &#8211; Open Civic Data may be dormant at the moment, but the standard it largely adopted, Popolo (which I lead), is still actively used and developed, by governments, corporations and especially civil society.</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: tmcw		</title>
		<link>https://civic.io/2016/01/28/the-future-of-civic-tech/#comment-10724</link>

		<dc:creator><![CDATA[tmcw]]></dc:creator>
		<pubDate>Sat, 30 Jan 2016 21:55:36 +0000</pubDate>
		<guid isPermaLink="false">http://civic.io/?p=4197#comment-10724</guid>

					<description><![CDATA[GTFS is something of an awkward poster-child for civic data standards. It was originally &quot;Google Transit Feed Specification&quot;, and was then renamed to &quot;General&quot; after the fact. While it&#039;s widespread, it&#039;s idiosyncratic enough that using it feels a lot like imitating Portland, not using a well-designed standard. That said, it was a tremendously successful collaboration between Google &#038; Trimet, and is very useful.

The role of standards bodies seems to have transitioned from &#039;where standards come from&#039; to really a place of mediation and semi-democracy between the orgs/cities/companies/people who write standards. In this pattern it&#039;s less of a &quot;why aren&#039;t standards bodies making something&quot; than &quot;how do we crystalize what we&#039;re doing and bring it to a standards body to synchronize it with everyone else&#039;s technique?&quot;

Finally, I think that requirements for usage or creation of standards can paralyze open data efforts. Publishing in Excel, CSV, JSON, without a domain-specific standard on top of it makes it less convenient to use data, but never impossible. If the information is there and the format is either simple or standardized on some level, it can be transformed into any other format. The question of how closely the standard should match the domain is one of degrees - JSON is general, a 311 standard is specific, a health report standard is even more specific. The more specific, the less general, and the more chance you have of overfitting the data with a strict standard based on a different city&#039;s needs.]]></description>
			<content:encoded><![CDATA[<p>GTFS is something of an awkward poster-child for civic data standards. It was originally &#8220;Google Transit Feed Specification&#8221;, and was then renamed to &#8220;General&#8221; after the fact. While it&#8217;s widespread, it&#8217;s idiosyncratic enough that using it feels a lot like imitating Portland, not using a well-designed standard. That said, it was a tremendously successful collaboration between Google &amp; Trimet, and is very useful.</p>
<p>The role of standards bodies seems to have transitioned from &#8216;where standards come from&#8217; to really a place of mediation and semi-democracy between the orgs/cities/companies/people who write standards. In this pattern it&#8217;s less of a &#8220;why aren&#8217;t standards bodies making something&#8221; than &#8220;how do we crystalize what we&#8217;re doing and bring it to a standards body to synchronize it with everyone else&#8217;s technique?&#8221;</p>
<p>Finally, I think that requirements for usage or creation of standards can paralyze open data efforts. Publishing in Excel, CSV, JSON, without a domain-specific standard on top of it makes it less convenient to use data, but never impossible. If the information is there and the format is either simple or standardized on some level, it can be transformed into any other format. The question of how closely the standard should match the domain is one of degrees &#8211; JSON is general, a 311 standard is specific, a health report standard is even more specific. The more specific, the less general, and the more chance you have of overfitting the data with a strict standard based on a different city&#8217;s needs.</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: mheadd		</title>
		<link>https://civic.io/2016/01/28/the-future-of-civic-tech/#comment-10722</link>

		<dc:creator><![CDATA[mheadd]]></dc:creator>
		<pubDate>Thu, 28 Jan 2016 18:24:19 +0000</pubDate>
		<guid isPermaLink="false">http://civic.io/?p=4197#comment-10722</guid>

					<description><![CDATA[An interesting point that was not included in this post, but that has surfaced in a &lt;a href=&quot;https://twitter.com/chris_whong/status/692760398760677377&quot; rel=&quot;nofollow&quot;&gt;subsequent Twitter conversation&lt;/a&gt; is that the primary driver of civic data standards (differences in the same kinds of data between different governments) is often a reflection of differences in an underlying government process.

Different jurisdictions use different processes to inspect restaurants, issue permits, enforce parking regulations, license rental properties, etc. And these differences bubble up to the data that these processes throw off. The process of developing a data standard then can be seen as a way to try and reconcile (obscure?) the differences between the data generated by the same kind of process in various jurisdictions, allowing for easier interoperability and comparability.

It strikes me that the difference in process we see between different jurisdictions in the operation of government can be viewed as a good thing - even a &lt;em&gt;desirable&lt;/em&gt; one - in our federalist system. Variations in how local governments operate demonstrate responsiveness to local needs / values, and can provides choice for citizens and taxpayers. If you don&#039;t like how jurisdiction X does something, you have the option to move to jurisdiction Y where they do it differently.

But from a technology standpoint, this is suboptimal. Differences in data need to be reconciled, and different data formats need to be parsed to use in a single app that may serve multiple jurisdictions. In the world of technology, differences - in operating systems, browser versions or data formats - can add complexity and increase the risk of a software issue.

It&#039;s worth noting this issue as something that might make developing open data standards more complicated than developing other kinds of standards. A sort of civic paradox - a scenario that produces an outcome that can be viewed as favorable from a democratic governance perspective, but lousy from a technology perspective.]]></description>
			<content:encoded><![CDATA[<p>An interesting point that was not included in this post, but that has surfaced in a <a href="https://twitter.com/chris_whong/status/692760398760677377" rel="nofollow">subsequent Twitter conversation</a> is that the primary driver of civic data standards (differences in the same kinds of data between different governments) is often a reflection of differences in an underlying government process.</p>
<p>Different jurisdictions use different processes to inspect restaurants, issue permits, enforce parking regulations, license rental properties, etc. And these differences bubble up to the data that these processes throw off. The process of developing a data standard then can be seen as a way to try and reconcile (obscure?) the differences between the data generated by the same kind of process in various jurisdictions, allowing for easier interoperability and comparability.</p>
<p>It strikes me that the difference in process we see between different jurisdictions in the operation of government can be viewed as a good thing &#8211; even a <em>desirable</em> one &#8211; in our federalist system. Variations in how local governments operate demonstrate responsiveness to local needs / values, and can provides choice for citizens and taxpayers. If you don&#8217;t like how jurisdiction X does something, you have the option to move to jurisdiction Y where they do it differently.</p>
<p>But from a technology standpoint, this is suboptimal. Differences in data need to be reconciled, and different data formats need to be parsed to use in a single app that may serve multiple jurisdictions. In the world of technology, differences &#8211; in operating systems, browser versions or data formats &#8211; can add complexity and increase the risk of a software issue.</p>
<p>It&#8217;s worth noting this issue as something that might make developing open data standards more complicated than developing other kinds of standards. A sort of civic paradox &#8211; a scenario that produces an outcome that can be viewed as favorable from a democratic governance perspective, but lousy from a technology perspective.</p>
]]></content:encoded>
		
			</item>
	</channel>
</rss>
